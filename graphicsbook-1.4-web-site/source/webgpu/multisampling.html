<!DOCTYPE html>
<head>
<!--
   Shows an animation of colored disks moving around in the canvas and
   bouncing off the edges.  The user can turn the animation on and off.

   This program is a small modification of instanced_draw.html that adds
   multisampling.  That is, instead of evaluating the fragment shader once
   for each pixel to get the color of that pixel, it evaluates it several
   times at different points within the pixel and averages the result.
   The only visible difference between the images produced by the two
   versions is that the edges of the disks drawn by this version are
   smoother than the edges of the disks in the previous version.  The effect
   is subtle, but should be noticeable if you look closely.  Multisampling
   is a kind of anti-aliasing; it can reduce the "jaggies" in an image.
   
   The only comments in this program are for explaining the changes from the
   previous version that are needed to implement multisampling.
-->   
<title>Basic WebGPU Example: Multisampling</title>
<style>
   body { 
      background-color: white;
   }
   #canvasholder {
      border: 2px solid black;
      display: inline-block;
   }
   canvas {
      display: block;
   }
</style>
<script>

"use strict";

const shaderSource = `
   
   struct VertexOutput {
       @builtin(position) position : vec4f,
       @location(0) color : vec4f
   }
   @vertex
   fn vertexMain( 
          @location(0) coords : vec2f,
          @location(1) offset : vec2f,
          @location(2) color : vec3f ) -> VertexOutput {
      var output : VertexOutput;
      output.position = vec4f( coords + offset, 0, 1 );
      output.color = vec4f(color,1);
      return output;
   }
   
   @fragment
   fn fragmentMain( @location(0) color : vec4f) -> @location(0) vec4f{
      return color;
   }
`;

const VERTEX_COUNT = 32;
const INSTANCE_COUNT = 50;

let vertexCoords = new Float32Array(2*VERTEX_COUNT);
let instanceOffsets = new Float32Array(2*INSTANCE_COUNT);
let instanceColors = new Float32Array(3*INSTANCE_COUNT);
let velocities = new Float32Array(2*INSTANCE_COUNT);

let context;
let device; 

let shader;
let pipeline;

let vertexBuffer;

let instanceOffsetBuffer;
let instanceColorBuffer;

let textureForMultisampling;      // A texture, to be created in createRenderPipeline().
let textureViewForMultisampling;  // A view of the texture, to be used in draw().
     // A texture in WebGPU is a resource on the GPU containing data for pixels
     // which can include, for example, mipmaps (reduced size copies of the image).
     // A view of the texture is a piece of the texture that can actually be
     // drawn to. (It might, for example, be only a subrectangular region in
     // one of the mipmaps.)  A view can be used in a color attachment to receive
     // the output from a fragment shader.  Multisampling in WebGPU requires an
     // extra texture view (in addition to the texture view for the canvas).
     // I do not understand why this is the case.


async function initWebGPU() {

   if (!navigator.gpu) {
      throw Error("WebGPU not supported in this browser.");
   }
   let adapter = await navigator.gpu.requestAdapter();
   if (!adapter) {
      throw Error("WebGPU is supported, but couldn't get WebGPU adapter.");
   }

   device = await adapter.requestDevice();
   
   let canvas = document.getElementById("webgpuCanvas");
   context = canvas.getContext("webgpu");
   context.configure({
      device: device,
      format: navigator.gpu.getPreferredCanvasFormat(),
      alphaMode: "premultiplied"
   });

   device.pushErrorScope("validation");
   shader = device.createShaderModule({
      code: shaderSource
   });
   let error = await device.popErrorScope();
   if (error) {
      throw Error("Compilation error in shader; see Console for details.");
   }   
}



function createPipelineConfig() {

   let vertexBufferLayout = [
      { 
         attributes: [ { shaderLocation:0, offset:0, format: "float32x2" } ],
         arrayStride: 8,
         stepMode: "vertex" 
      },
      { 
         attributes: [ { shaderLocation:1, offset:0, format: "float32x2" } ],
         arrayStride: 8,
         stepMode: "instance" 
      },
      {
         attributes: [ { shaderLocation:2, offset:0, format: "float32x3" } ],
         arrayStride: 12,
         stepMode: "instance" 
      }
   ];
      
   let pipelineDescriptor = {
       vertex: {
          module: shader,
          entryPoint: "vertexMain",
          buffers: vertexBufferLayout
       },
       fragment: {
          module: shader,
          entryPoint: "fragmentMain",
          targets: [{
            format: navigator.gpu.getPreferredCanvasFormat()
          }]
       },
       primitive: {
          topology: "triangle-strip"
       },
       multisample: {  // Sets number of samples for multisampling
         count: 4,     //  (4 and 1 are currently the only possible values).
       },
       layout: "auto"
   };
    
   pipeline = device.createRenderPipeline(pipelineDescriptor);  
   
   vertexBuffer = device.createBuffer({ 
       size: vertexCoords.byteLength, 
       usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST
   });   
   device.queue.writeBuffer(vertexBuffer, 0, vertexCoords); 
   
   instanceOffsetBuffer =  device.createBuffer({ 
       size: instanceOffsets.byteLength, 
       usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST
   });   
   device.queue.writeBuffer(instanceOffsetBuffer, 0, instanceOffsets); 
   
   instanceColorBuffer =  device.createBuffer({ 
       size: instanceColors.byteLength, 
       usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST
   });   
   device.queue.writeBuffer(instanceColorBuffer, 0, instanceColors);
   
   /* Create the texture and texture view that are required for
    * multisampling.  The same texture view can be used in all calls
    * to the draw() function, so it can be created here just
    * once. */
   
   textureForMultisampling = device.createTexture({
      size: [context.canvas.width, context.canvas.height],
      sampleCount: 4,  // (4 is actually the only value.)
      format: navigator.gpu.getPreferredCanvasFormat(),
      usage: GPUTextureUsage.RENDER_ATTACHMENT,
    });
    textureViewForMultisampling = textureForMultisampling.createView();
}


function initializeDataArrays() {

   vertexCoords[0] = 0.1;
   vertexCoords[1] = 0;
   for (let i = 1; i <= VERTEX_COUNT/2 - 1; i++) {
      let angle = 2*Math.PI/VERTEX_COUNT * i
      vertexCoords[4*(i-1)+2] = 0.1 * Math.cos(angle);
      vertexCoords[4*(i-1)+3] = -0.1 * Math.sin(angle);
      vertexCoords[4*(i-1)+4] = 0.1 * Math.cos(angle);
      vertexCoords[4*(i-1)+5] = 0.1 * Math.sin(angle);
   }
   vertexCoords[2*VERTEX_COUNT-2] = -0.1;
   vertexCoords[2*VERTEX_COUNT-1] = 0;
   
   for (let i = 0; i < INSTANCE_COUNT; i++) {
       instanceOffsets[2*i] = 1.8*Math.random() - 0.9;
       instanceOffsets[2*i+1] = 1.8*Math.random() - 0.9;
       let speed = 0.1 + 0.5*Math.random();
       let theta = 2*Math.PI*Math.random();
       velocities[2*i] = speed*Math.cos(theta);
       velocities[2*i+1] = speed*Math.sin(theta);
       instanceColors[3*i] = Math.random();
       instanceColors[3*i+1] = Math.random();
       instanceColors[3*i+2] = Math.random();
   }   
}


function draw() {
   let commandEncoder = device.createCommandEncoder();
   let renderPassDescriptor = {
      colorAttachments: [{
          clearValue: { r: 0.9, g: 0.9, b: 0.9, a: 1 }, 
          loadOp: "clear", 
          storeOp: "store", 
          view: textureViewForMultisampling, // For rendering to the multisampling texture.
          resolveTarget: context.getCurrentTexture().createView() // For the final image.
              // The change from the non-multisampled version is that
              // the "view" for the color attachment is is now set
              // to the view from the multisampling texture, and a
              // resolveTarget property whose value is the texture view
              // for the canvas.  (In the previous version, the canvas texture was
              // used directly as the "view" property of the previous version.)
      }]
   };
   let passEncoder = commandEncoder.beginRenderPass(renderPassDescriptor);
   passEncoder.setPipeline(pipeline);
   passEncoder.setVertexBuffer(0,vertexBuffer);
   passEncoder.setVertexBuffer(1,instanceOffsetBuffer);
   passEncoder.setVertexBuffer(2,instanceColorBuffer);
   passEncoder.draw(VERTEX_COUNT,INSTANCE_COUNT);
   passEncoder.end();
   let commandBuffer = commandEncoder.finish();
   device.queue.submit([commandBuffer]);
}


let running = false;
let previousTime;


function doFrame() {
   if (!running)
      return;
   let now = performance.now();
   let dt = (now - previousTime)/1000;
   previousTime = now;
   for (let i = 0; i < INSTANCE_COUNT; i++) {
       let x = 2*i;
       let y = 2*i+1;
       instanceOffsets[x] += velocities[x] * dt;
       instanceOffsets[y] += velocities[y] * dt;
       if (instanceOffsets[x] > 0.9) {
           velocities[x] = -velocities[x];
           instanceOffsets[x] = 1.8 - instanceOffsets[x];
       }
       else if (instanceOffsets[x] < -0.9) {
           velocities[x] = -velocities[x];
           instanceOffsets[x] = -1.8 - instanceOffsets[x];
       }
       if (instanceOffsets[y] > 0.9) {
           velocities[y] = -velocities[y];
           instanceOffsets[y] = 1.8 - instanceOffsets[y];
       }
       else if (instanceOffsets[y] < -0.9) {
           velocities[y] = -velocities[y];
           instanceOffsets[y] = -1.8 - instanceOffsets[y];
       }
   }
   device.queue.writeBuffer(instanceOffsetBuffer,0,instanceOffsets);
   draw();
   requestAnimationFrame(doFrame);
}


function doAnimateCheckbox() {
   let anim = document.getElementById("anim").checked;
   if (anim == running)
      return;
   running = anim;
   if (running) { // Animation is being started.
      previousTime = performance.now();
      requestAnimationFrame(doFrame);
   }
}
   
async function init() {
   try {
      await initWebGPU();
      initializeDataArrays();
      createPipelineConfig();
   }
   catch (e) {
       document.getElementById("message").innerHTML =
          "<span style='color:#AA0000; font-size:110%'><b>Error: Could not initialize WebGPU: </b>" + 
                    e.message + "</span>";
       return;
   }
   draw();
   document.getElementById("anim").checked = false;
   document.getElementById("anim").onchange = doAnimateCheckbox;
}

window.onload = init;

</script>
</head>

<body>

<h2>Basic WebGPU Example (Multisampling)</h2>

<noscript><h3>Error: This page requires JavaScript and a web browser that supports WebGPU!</h3><hr></noscript>

<p id="message"></p>

<p><label><input type=checkbox id="anim"> <b>Animate</b></label>

<div id="canvasholder">
<canvas width=500 height=500 id="webgpuCanvas"></canvas>
</div>

</body>
</html>

