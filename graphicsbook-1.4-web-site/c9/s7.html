<!DOCTYPE html>
<html>
<head>
<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Introduction to Computer Graphics, Section 9.7 -- Some Details</title>
<link type="text/css" rel="stylesheet" href="../resource/graphicstext.css">
</head>
<body>
<div class="page">
<div align="right">
<small>
        [  <a href="s6.html">Previous Section</a> |
           <a href="index.html">Chapter Index</a> | 
	    <a href="../index.html">Main Index</a> ]
    </small>
</div>
<hr>
<table class="subsections" cellpadding="5" border="2">
<tr>
<td>
<div align="center">
<b>Subsections</b>
<hr>
<small><a href="#webgpu.7.1">Lost Device</a>
<br>
<a href="#webgpu.7.2">Error Handling</a>
<br>
<a href="#webgpu.7.3">Limits and Features</a>
<br>
<a href="#webgpu.7.4">Render Pass Options</a>
<br>
<a href="#webgpu.7.5">Render Pipeline Options</a>
<br>
</small>
</div>
</td>
</tr>
</table>
<div class="content section">
<h3 class="section_title">Section 9.7</h3>
<h2 class="section_title">Some Details</h2>
<hr class="break">


<p class="firstpar">To finish this introduction to WebGPU, we'll look briefly at a few useful things 
to know that  didn't make it into earlier sections.  You will find several new sample
programs in the last two subsections.</p>



<div class="subsection">
<hr class="break">
<h3 class="subsection_title" id="webgpu.7.1">9.7.1&nbsp;&nbsp;Lost Device</h3>


<p>If you start writing more serious applications, you should be aware
that it's possible for a WebGPU device to become "lost."  When that happens,
the device stops working, and anything that you try to do with the device
will be ignored.  Resources such as buffers and pipelines that were 
created with the device will no longer be valid.
Ordinarily, this will be rare. It can happen, for example, 
if you call <span class="code">device.destroy()</span> because you no longer need the
device.  It could happen if the user unplugs an external display.
More disturbing, the WebGPU specification says, "The device may become lost 
if shader execution does not end  in a reasonable amount of time, as determined
by the user agent."  The "user agent" is the web browser that is running
your program.  That does not give much definite guidance about what to 
expect.</p>


<p>The function <span class="code">device.lost()</span> returns a <span class="word" data-term="promise (in JavaScript)" data-definition="In JavaScript programming, a promise represents a result
that might be available immediately or at some time in the future.  A programmer can provide
a function to be called if and when the promise is fulfilled (that is when the result becomes
available).  A programmer can also provide a function to be called when the promise is
rejected (for example, if some error occurs).  Promises are asynchronous since the function
that handles success or failure will be called at some unpredictable time." title="Click for a definition of promise (in JavaScript).">promise</span>
that resolves if and when the device becomes lost.  It can be used to set up
a function to be called if the device is lost.  It might be used
something like this, just after creating the device:</p>


<pre>device.lost().then(
   (info) =&gt; {
      if ( info.reason !== "destroyed" ) {
         ... // (possibly try to recover)
      }
   }
);</pre>


<p class="noindent">The only possible values of <span class="code">info.reason</span> are
"destroyed" (meaning that <span class="code">device.destroy()</span> was called)
and "unknown."  If the reason is not "destroyed," you might try to
create a new device and reinitialize your application&mdash;at the
risk that the same thing will go wrong again.</p>


<p>Hopefully, the behavior of <span class="code">device.lost()</span> will be better
defined in the future.</p>


</div>




<div class="subsection">
<hr class="break">
<h3 class="subsection_title" id="webgpu.7.2">9.7.2&nbsp;&nbsp;Error Handling</h3>


<p>The first thing to remember about WebGPU errors is that they will almost always
be reported in the Web browser's console.  WebGPU error messages are informative and
will often give you hints about how to fix the problem.  The second thing to know 
is that WebGPU validates programs according to tightly specified criteria.  If a
program passes validity checks on one platform, it is likely to do so on every
platform.  The third thing is that when WebGPU finds a validity error, it does
not automatically stop processing.  It will mark the object that caused the
problem as invalid and will try to continue.  Attempts to use the invalid
object will produce more error messages.  So, if your program produces a
series of error messages, concentrate on the first one.</p>


<p>You can improve the error messages generated by WebGPU by labeling your objects.
You can label just about any WebGPU object with a text string of your choosing
by adding a <span class="code">label</span> property to the object.  If WebGPU finds a
validation error in the object, it will include the label in the error message.
For example, if your program uses several bind groups and
one of them causes a problem, adding labels to your bind groups can help you
track down the error:</p>


<pre>bindGroupA = device.createBindGroup({
    <span class="newcode">label: "bind group for outlines",</span>
    layout: 
       .
       .
       .</pre>


<p>Instead of relying on the Web browser console, it is possible to have a
program check for errors.  Things are complicated by the fact that errors
are detected by the GPU side of the program.  To get the error report
back to the JavaScript side, you can use <span class="code">device.pushErrorScope()</span>
to add an error check to the GPU.  Later, you can retrieve the result
by calling <span class="code">device.popErrorScope()</span>. <span class="code">pushErrorScope()</span>
takes a parameter indicating the type of error that you want to detect.
The parameter can be "validation", "out-of-memory", or "internal";
"validation" is the most common.  <span class="code">popErrorScope()</span> returns
a promise that resolves when all operations submitted to the GPU after
the corresponding push have been completed.  The value returned by
the promise will be <span class="code">null</span> if no error was detected; otherwise,
it will be an object with a <span class="code">message</span> property that
describes the error.</p>


<p>For example, when I am developing a program, I like to check for
compilation errors in my <span class="word" data-term="shader" data-definition="A program to be executed at some stage of the rendering pipeline.  OpenGL
shaders are written in the GLSL programming languages.  For WebGL, only vertex shaders
and fragment shaders are supported.  WebGPU also has compute shaders, which are used
in compute pipelines." title="Click for a definition of shader.">shader</span> code.  I can do that
by pushing a "validation" error scope before attempting the
compilation:</p>


<pre>device.pushErrorScope("validation");
shader = device.createShaderModule({
   code: shaderSource
});
let error = await device.popErrorScope();
if (error) {
   throw Error("Compilation error in shader: " + error.message);
}</pre>


<p class="noindent">The error check could be removed once the program is working.</p>


<p>When WebGPU encounters an error that is not captured by an error
scope, it generates an "uncapturederror" event.  You can add an
event handler to the device to respond to uncaptured errors:
<span class="code">device.onuncapturederror = function(event) { ... }</span>.
But, as always, remember that watching the Web browser console is
usually good enough!</p>


</div>




<div class="subsection">
<hr class="break">
<h3 class="subsection_title" id="webgpu.7.3">9.7.3&nbsp;&nbsp;Limits and Features</h3>


<p>A WebGPU device is subject to certain "limits," such as the maximum number of
<span class="word" data-term="vertex buffer" data-definition="In WebGPU, a vertex buffer is a GPU data structure that
holds values to be used as input the vertex shader." title="Click for a definition of vertex buffer.">vertex buffers</span> that can be attached to a render 
pipeline or the maximum size of a compute workgroup.  When you create a device
by calling <span class="code">adapter.requestDevice()</span> with no parameter, the device
that is returned has a default set of limits which are guaranteed to be supported
by every WebGPU implementation.  For example, the default maximum size for a
workgroup is 256.  For most applications, the default limits are fine.
However, if you absolutely need a workgroup of size 1024, you can try requesting
a device with that limit:</p>


<pre>device = await adapter.requestDevice({
    requiredLimits: {
       maxComputeInvocationsPerWorkgroup: 1024
    }
});</pre>


<p class="noindent">If the WebGPU adapter doesn't support the requested limit, this will
throw an exception.  If it succeeds in your Web browser, it means that you
are writing a program that might fail elsewhere, when run on a platform
that doesn't support the increased limit.</p>


<p>The object <span class="code">adapter.limits</span> contains the actual limits supported
by the adapter. (To see a list, write the object to the console.)  Before 
requesting an increased limit, you should check this object to see whether
the adapter supports it.</p>


<p>WebGPU also defines a set of "features," which represent optional device
capabilities.  For example, the feature "texture-compression-bc" makes it possible
to use a certain type of compressed texture.  (Compressed textures are not covered
in this book.)  Features cannot be used unless they are requested when
the device is created:</p>


<pre>device = await adapter.requestDevice({
   requiredFeatures: ["texture-compression-bc"] // array of feature names
});</pre>


<p class="noindent">Again, this will throw an exception if the feature is not available, 
and a feature request will limit the devices on which your program can run.
The boolean-valued function <span class="code">adapter.hasFeature(name)</span> 
can be used to test whether the adapter supports the feature wih the
given <span class="code">name</span>.  For a list of possible features, see the
WebGPU documentation.</p>



</div>




<div class="subsection">
<hr class="break">
<h3 class="subsection_title" id="webgpu.7.4">9.7.4&nbsp;&nbsp;Render Pass Options</h3>


<p>A render pass encoder is used to add drawing commands to a command encoder.  It
specifies a pipeline and resources such as bind groups that are required by the
pipeline.  It also has several other options.  We'll look at two of them here.</p>


<p>The <span class="word" data-term="viewport" data-definition="The rectangular area in which the image for 2D or 3D graphics is 
displayed.  The coordinates on the viewport are pixel coordinates, more properly called 
device coordinates since they are actual physical coordinates on the device where the 
image is being displayed." title="Click for a definition of viewport.">viewport</span> is the rectangular region in a canvas or other
render target in which the rendered image is displayed.  The default viewport
is the entire render target, but the <span class="code">setViewport()</span> function in
a render pass encoder can be used to select a smaller viewport.  The standard
WebGPU <span class="word" data-term="NDC" data-definition="Normalized Device Coordinates.  In WebGPU, refers to the default 
xyz coordinate system in which x and y range from -1 to&nbsp;1 and z ranges
from 0 to&nbsp;1.  The x and y in NDC map linearly to device, or pixel, coordinates
on the viewport." title="Click for a definition of NDC.">NDC</span> coordinate system, with x and y ranging from minus one to
one and depth ranging from zero to one, is then mapped onto the smaller viewport, 
and no drawing takes place outside that viewport.  If <span class="code">passEncoder</span>
is a render pass encoder, a call to the function takes the form</p>


<pre>passEncoder.setViewport( left, top, width, height, depthMin, depthMax );</pre>


<p class="noindent">where <span class="code">left</span>, <span class="code">top</span>, <span class="code">width</span>, and
<span class="code">height</span> are given in pixel coordinates, and <span class="code">depthMin</span>
and <span class="code">depthMax</span> are in the range 0 to 1, with <span class="code">depthMin</span>
less than <span class="code">depthMax</span>.  Usually, <span class="code">depthMin</span> will be
zero and <span class="code">depthMax</span> will be one.  For example, when drawing to
an 800-by-600 pixel canvas, you can map the scene to the right half of
the canvas using</p>


<pre>passEncoder.setViewport( 400, 0, 400, 600, 0, 1 );</pre>


<p>In addition, you can restrict drawing to a smaller rectangle within the
viewport using <span class="code">setScissorRect()</span>, which has the
form</p>


<pre>passEncoder.setScissorRect( left, top, width, height );</pre>


<p class="noindent">where again <span class="code">left</span>, <span class="code">top</span>, <span class="code">width</span>, and
<span class="code">height</span> are given in pixel coordinates.  The difference between
viewport and scissor rect is that a scissor rect does not affect the coordinate
mapping:  The viewport shows the entire rendered scene, but a scissor rect
prevents part of the scene from being drawn.</p>


<p>The sample program <span class="sourceref"><a href="../source/webgpu/viewport_and_scissor.html">webgpu/viewport_and_scissor.html</a></span> uses
both viewport and scissor rect.  It is yet another moving disk animation, showing
colored disks with black outlines.  Different viewports are used to draw four
copies of the scene to the four quadrants of a canvas.  In two of the viewports,
a scissor rect is also applied, but just to the disk interiors, not to their
outlines.</p>



</div>




<div class="subsection">
<hr class="break">
<h3 class="subsection_title" id="webgpu.7.5">9.7.5&nbsp;&nbsp;Render Pipeline Options</h3>


<p>A pipeline descriptor is used with <span class="code">device.createRenderPipeline()</span>
to create a render <span class="word" data-term="pipeline" data-definition="A sequence of computational stages in a GPU that are applied to
incoming data to produce some result.  Some of the stages can be programmable shaders,
such as vertex shaders, fragment shaders, and compute shaders.  In a graphics rendering 
pipeline, the output is the colors of the pixels in an image." title="Click for a definition of pipeline.">pipeline</span>. The descriptor has a number of options
that affect how the pipeline will <span class="word" data-term="rendering" data-definition="The process of producing a 2D image from a 3D scene description." title="Click for a definition of rendering.">render</span>
<span class="word" data-term="geometric primitive" data-definition="Geometric objects in a graphics system, such as OpenGL, that are
not made up of simpler objects.  Examples in OpenGL include points, lines, and triangles,
but the set of available primitives depends on the graphics system.  (Note that as the term
is used in OpenGL, a single primitive can be made up of many points, line segments, or triangles.)" title="Click for a definition of geometric primitive.">primitives</span>.  We have seen, for example,
how the <span class="code">multisample</span> property is used for multisampling
<span class="word" data-term="antialiasing" data-definition="A technique used to reduce the jagged or &quot;staircase&quot; appearance
of diagonal lines, text, and other shapes that are drawn using pixels.  When a pixel is only partly
covered by a geometric shape, then the color of the pixel is a blend of the color of the shape and
the color of the background, with the degree of blending depending on the fraction of the
pixel that is covered by the geometric shape." title="Click for a definition of antialiasing.">antialiasing</span> (<a href="../c9/s2.html#webgpu.2.5">Subsection&nbsp;9.2.5</a>) and how <span class="code">detpthStencil</span>
is used to configure the <span class="word" data-term="depth test" data-definition="A solution to the hidden surface problem that involves keeping
track of the depth, or distance from the viewer, of the object currently visible at each
pixel in the image.  When a new object is drawn at a pixel, the depth of the new object
is compared to the depth of the current object to decide which one is closer to the viewer.
The advantage of the depth test is that objects can be rendered in any order.  A disadvantage
is that only a limited range of depths can be represented in the image." title="Click for a definition of depth test.">depth test</span> (<a href="../c9/s4.html#webgpu.4.1">Subsection&nbsp;9.4.1</a>).
Here, we look at a few more render pipeline options.</p>


<p>
<b>Color Blending.</b>  By default, the color that is output by a fragment
shader replaces the current color of the fragment.  But it is possible for the
two colors to be blended.  That is, the new color of the fragment will be
some combination of the "source" color (from the shader) and the "destination"
color (the current color of the fragment in the render target).  This is
often used to implement translucent colors, where the alpha component of
the source color determines the degree of transparency.  For an example,
see the sample program <span class="sourceref"><a href="../source/webgpu/alpha_blend.html">webgpu/alpha_blend.html</a></span>.</p>


<p>The configuration for color blending is nested inside the <span class="code">fragment</span>
property of the pipeline descriptor.  The functionality is similar to the
<span class="word" data-term="WebGL" data-definition="A 3D graphics API for use on web pages.  WebGL programs are written
in the JavaScript programming language and display their images in HTML canvas
elements.  WebGL is based on OpenGL ES, the version of OpenGL for embedded systems, with
a few changes to adapt it to the JavaScript language and the Web environment." title="Click for a definition of WebGL.">WebGL</span> function <span class="code">gl.blendFuncSeparate()</span>, which is
discussed in <a href="../c7/s4.html#webgl3d.4.1">Subsection&nbsp;7.4.1</a>.  Here is the typical configuration
for translucency:</p>


<pre>fragment: {
   module: shader,
   entryPoint: "fragmentMainForDisk",
   targets: [{
     format: navigator.gpu.getPreferredCanvasFormat(),
     blend: { // Configure the formulas to be used for color blending.
        color: { // For RGB color components.
           operation: "add",                  // "add" is the default.
           srcFactor: "src-alpha",            // The default is "one".
           dstFactor: "one-minus-src-alpha"   // The default is "zero".
        },
        alpha: { // For the alpha component.
           operation: "add",
           srcFactor: "zero",
           dstFactor: "one"
        }
     }
   }]
}</pre>


<p class="noindent">Blending for the red, green, and blue color components is configured
separately from the alpha component.  The values used here for the <span class="code">color</span> property
say that the new RGB color value is a weighted average of the fragment shader output
and the current fragment color.  The values used for <span class="code">alpha</span>
say that the alpha component of the destination will remain unchanged.
The general formula, using the "add" <span class="code">operation</span>, is</p>


<pre>new_color = shader_output*srcFactor + current_color*dstFactor</pre>


<p class="noindent">Another common configuration is to set the <span class="code">operation</span> to "add"
and both <span class="code">srcFactor</span> and <span class="code">dstFactor</span> to "one",
meaning that the shader output is simply added to the current color.
This might be used to build up the colors in the target by using multiple
passes that each add a little to the color value.</p>




<p>
<b>Color Masking.</b>  The <span class="code">writeMask</span> property of the fragment target
lets you control which color components of the fragment shader output will be written
to the render target.  (The same functionality is called "color masking" in OpenGL;
<a href="../c7/s4.html#webgl3d.4.1">Subsection&nbsp;7.4.1</a> discusses how it can be used for <span class="word" data-term="anaglyph stereo" data-definition="A technique for combining stereographic images of a scene, one for the
left eye and one for the right eye, into a single image.  Typically, the image for the left eye is
drawn using only shades of red, and the image for the right eye contains only blue and green
color components.  The 3D effect can be seen by viewing the combined image through
red/cyan glasses, which allow each eye to see only the image that is intended for that eye." title="Click for a definition of anaglyph stereo.">anaglyph stereo</span>.)
For example, if you restrict writing to the red component,
then only the red component of the current fragment color can be changed; the
green, blue, and alpha components will be left unchanged.  Here is how you
would do that in a render pipeline descriptor:</p>


<pre>fragment: {
   module: shader,
   entryPoint: "fragmentMain",
   targets: [{
     format: navigator.gpu.getPreferredCanvasFormat(),
     writeMask: GPUColorWrite.RED  // Only write the red component to target.
   }]
}</pre>


<p class="noindent">Other values for the <span class="code">writeMask</span> property include
<span class="code">GPUColorWrite.GREEN</span>, <span class="code">GPUColorWrite.BLUE</span>, and
<span class="code">GPUColorWrite.ALPHA</span>.  You can also combine several
of these constants with the or ("<span class="code">|</span>") operator to write several
components.  For example,</p>


<pre>writeMask: GPUColorWrite.GREEN | GPUColorWrite.BLUE</pre>


<p class="noindent">The default value is <span class="code">GPUColorWrite.ALL</span>, which means that all
four color components are written.  The sample program <span class="sourceref"><a href="../source/webgpu/color_mask.html">webgpu/color_mask.html</a></span>
lets you experiment with writing to any combination of the red, green, and blue
color components.  Note that if you write just the red component to a black background,
you will get shades of red, since the green and blue components will still be
zero after writing.  But if you write to a white background, you will get shades
of blue-green, since the green and blue components will still equal one after the write,
while the red component can be less than one.</p>



<p>
<b>Depth Bias.</b>  When the depth test is enabled, drawing two things at almost
exactly the same depth can be a problem, because one object might be visible at
some pixels while the other object is visible at other pixels.  See the end of
<a href="../c3/s1.html#gl1geom.1.4">Subsection&nbsp;3.1.4</a>.  The solution is to add a small amount, or "bias,"
to the depth of one of the objects.  (This is called "polygon offset" in OpenGL; 
see the end of <a href="../c3/s4.html#gl1geom.4.1">Subsection&nbsp;3.4.1</a>.)  The sample program
<span class="sourceref"><a href="../source/webgpu/polyhedra.html">webgpu/polyhedra.html</a></span> lets the users view <span class="word" data-term="polyhedron" data-definition="A closed 3D figure whose faces, or sides, are polygons.  Usually, it is
assumed that the faces of a polyhedron do not intersect, except along their edges." title="Click for a definition of polyhedron.">polyhedra</span>
that are drawn with white faces and black edges.  It uses depth bias to ensure that
the edges are fully visible.  The configuration is part of the <span class="code">depthStencil</span>
property of the pipeline descriptor that is used for drawing the faces:</p>


<pre>depthStencil: {  
   depthWriteEnabled: true,
   depthCompare: "less",
   format: "depth24plus",
   depthBias: 1,
   depthBiasSlopeScale: 1.0
}</pre>


<p class="noindent">The <span class="code">depthBias</span> and <span class="code">depthBiasSlopeScale</span> properties are used
to modify the depth of each fragment that is rendered by the pipeline.  The default
values are zero, which leaves the depth unchanged.  Positive values will increase
the fragment's depth, moving it a bit away from the user.  The values 1 and 1.0 for
<span class="code">depthBias</span> and <span class="code">depthBiasSlopeScale</span> shown here should work
in most cases. (The value of
<span class="code">depthBias</span> is multiplied by the smallest positive difference between
two depths that can be represented in the <span class="word" data-term="depth buffer" data-definition="A region of memory that stores the information needed for the depth test
in 3D graphics, that is, a depth value for each pixel in the image.  Also called the &quot;z-buffer.&quot;" title="Click for a definition of depth buffer.">depth buffer</span>.  That by itself might
work in many cases, but for triangles that the user is viewing close to edge-on, it
might not be enough.  The <span class="code">depthBiasSlopeScale</span> adds an additional bias
that depends on the angle that the triangle makes with the view direction.)
Note that depth bias seems to work only for triangle primitives, not for lines or
points, so the depth bias in the sample program is applied to the faces of the
polyhedron, not to the edges.</p>



<p>
<b>Face Culling and Front Face</b>.  The polyhedra example uses two more
pipeline options: <span class="code">cullMode</span> and <span class="code">frontFace</span>.  The are
options in the <span class="code">primitive</span> property of the render pipeline descriptor.</p>


<p>The polyhedra in the program are all closed objects: The interior is
completely hidden by the exterior.  There is no need to render back-facing
polygons, since they lie behind front-facing polygons.  The <span class="code">cullMode</span>
property can be used to turn off rendering of either front-facing or back-facing
triangles.  With the default value, "none", no triangles are culled.  In the
polyhedra program, I set <span class="code">cullMode</span> to "back", to avoid the expense
of rendering back-facing triangles that would not be visible in the final image.</p>


<p>However, I had to make another change.  The usual convention is that the
front face of a triangle is determined by the rule that when looking at the
front face, the vertices are given in counterclockwise order.  However, the
polyhedron models in the program use the opposite convention: clockwise
ordering.  So, I set the <span class="code">frontFace</span> option of the primitive 
to "cw" to specify clockwise vertex ordering.
</p>


<pre> primitive: {
    topology: "triangle-list",
    cullMode: "back",  // Other values are "front" and "none".
    frontFace: "cw"    // The other value is "ccw" (counterclockwise).
}</pre>


<p class="noindent">Now, that change has no effect on the appearance of the scene;
it was done for efficiency only.  And if you wondering, yes, I could
have just set <span class="code">cullMode</span> to "front", but that would be
misleading&mdash;and it would have left me with no example for
<span class="code">frontFace</span>.</p>


</div>




</div>
<hr>
<div align="right">
<small>
        [  <a href="s6.html">Previous Section</a> |
           <a href="index.html">Chapter Index</a> | 
	    <a href="../index.html">Main Index</a> ]
    </small>
</div>
</div>
</body>
<script src="../resource/glossary.js"></script>
</html>
